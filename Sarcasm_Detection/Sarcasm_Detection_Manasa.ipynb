{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dce5001c",
   "metadata": {},
   "source": [
    "###       Sarcasm Detection Using Positional Encoding, Self_Attention and Multi-Head Attention\n",
    "\n",
    "## Project Overview :\n",
    "    \n",
    "    This project aims to develop a deep leaning model for detecting the Sarcasm in a given sentences. The model is built using Transformer-Encoder Block, which are well suited for the detecting the Sarcasm tasks. The project includes the following steps:\n",
    "\n",
    "1. Data Collection: We use the text of Sarcasm_Detection_Dataset_v2.json  dataset. This dataset has rich complex text provides a good challenge for out mode.\n",
    "\n",
    "2. Data Preprocessing: The text data is tokenized, converted into sequences, and padded to ensure unifrom input lengths. The sequences are then split into training and testing set.\n",
    "\n",
    "3. Model Building: An Transformer-Based Model is constructed with an\n",
    "\n",
    "  -> MultiHeadAttention\n",
    "  -> Positional Encoding\n",
    "  -> LayerNormalization\n",
    "  -> Feed-Forward Network\n",
    "\n",
    "4. Model Training: The model is trained using the prepared sequences.\n",
    "\n",
    "5. Model Evaluation: The model is evaluated using a set of example sentences to test its ability to detect whether the given sentence is sarcastic or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3de027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            headline  is_sarcastic\n",
      "0  thirtysomething scientists unveil doomsday clo...             1\n",
      "1  dem rep. totally nails why congress is falling...             0\n",
      "2  eat your veggies: 9 deliciously different recipes             0\n",
      "3  inclement weather prevents liar from getting t...             1\n",
      "4  mother comes pretty close to using word 'strea...             1\n"
     ]
    }
   ],
   "source": [
    "# Step-1 : Import the necessary libraries\n",
    "# Step-2 : Load the dataset from the JSON file\n",
    "import pandas as pd\n",
    "import json\n",
    "with open('Sarcasm_Headlines_Dataset_v2.json', 'r') as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "df = pd.DataFrame(data)\n",
    "df = df[['headline','is_sarcastic']]\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5994259b",
   "metadata": {},
   "source": [
    "### Code Explanation :\n",
    "\n",
    "-> import pandas : importing the library pandas.\n",
    "-> data = pd.DataFrame(data) : loads the dataset from the JSON file\n",
    "-> data.head() : Display the first few rows to inspect the structure of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6979ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step-3 : Clean the text data\n",
    "# Importing necessary libraries for text cleaning\n",
    "\n",
    "import re\n",
    "\n",
    "# Clean text function to remove unwanted characters and normalize\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "\n",
    "        # Remove all characters except alphabets and space\n",
    "        text = re.sub('[^a-zA-Z ]', ' ', text)\n",
    "\n",
    "        # Remove extra spaces\n",
    "        text = ' '.join(text.split())\n",
    "\n",
    "        # Convert to lowercase\n",
    "        return text.lower()\n",
    "    return \"\"\n",
    "\n",
    "# Apply cleaning to both input and output texts\n",
    "df['headline'] = df['headline'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44c8971",
   "metadata": {},
   "source": [
    "### Code Explanation :\n",
    "\n",
    " TEXT CLEANING + FORMATTING FOR SEQ2SEQ\n",
    "-> import re\n",
    "   -> Imports Python’s built-in Regular Expression (regex) module.\n",
    "   -> Reason : Required for pattern-based text substitution and cleaning (e.g., removing non-alphabet characters).\n",
    "   -> Purpose: Helps in cleaning the raw text by removing special characters, numbers, and extra whitespace\n",
    "\n",
    "-> def clean_text(text):\n",
    "   -> Defines a custom function named clean_text that takes a single argument text.\n",
    "   -> Reason : Modularizes the text cleaning process, so it can be reused on multiple text fields.\n",
    "   -> Purpose: To ensure all input and output text is cleaned in a consistent way before feeding it to the model.\n",
    "\n",
    "-> if isinstance(text, str):\n",
    "   -> Checks if text is a string.\n",
    "   -> Reason : Prevents errors if text is NaN or another non-string data type.\n",
    "   -> Purpose: Defensive programming — ensures cleaning is applied only on valid strings.\n",
    "\n",
    "-> text = re.sub('[^a-zA-Z ]', ' ', text)\n",
    "   -> Replaces everything except alphabets and spaces with a space.\n",
    "   -> Reason : Removes numbers, punctuation, special characters (e.g., .,?!@).\n",
    "   -> Purpose: Keeps the text simple and clean — only words. Models like LSTM/GRU perform better with cleaner data.\n",
    "\n",
    "-> text = ' '.join(text.split())\n",
    "   ->  Breaks the text into words (.split()), removes extra whitespace, and joins it back with single spaces.\n",
    "   -> Reason : Handles multiple spaces or irregular spacing.\n",
    "   -> Purpose: Ensures consistent word separation and formatting.\n",
    "\n",
    "-> return text.lower()\n",
    "   -> Converts all characters in the text to lowercase.\n",
    "   -> Reason : To reduce vocabulary size. E.g., India and india should be treated the same.\n",
    "   -> Purpose: Simplifies training and improves model generalization.\n",
    "\n",
    "-> return \"\"\n",
    "  ->  If the input text is not a string, return an empty string.\n",
    "  -> Prevents the function from failing on None or non-text inputs.\n",
    "  -> Purpose: Robustness.\n",
    "\n",
    "-> data['headline'] = data['headline'].apply(clean_text)\n",
    "   -> Applies clean_text() to every row in the headline column.\n",
    "   -> Reason : Prepares target output (headline) for model training.\n",
    "   -> Purpose: Ensures the decoder learns from clean data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbab181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\GEN_AI\\A_neural\\.venv\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "sequneces [[14808, 352, 3155, 6257, 2143, 2, 641, 1123], [7237, 1630, 732, 3156, 52, 233, 12, 1844, 984, 8, 1430, 1986, 1779], [872, 38, 10879, 14809, 618, 1478], [10880, 1533, 6258, 4519, 16, 145, 1, 147], [273, 486, 298, 923, 1, 565, 527, 3832, 6259]]\n",
      "padded_sequences [[    0     0     0 ...     2   641  1123]\n",
      " [    0     0     0 ...  1430  1986  1779]\n",
      " [    0     0     0 ... 14809   618  1478]\n",
      " [    0     0     0 ...   145     1   147]\n",
      " [    0     0     0 ...   527  3832  6259]]\n",
      "labels [1 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Step-4 : Tokenization and Padding\n",
    "# Importing necessary libraries for tokenization and padding\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['headline'])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index)+1\n",
    "\n",
    "max_sent_len = max([len(x) for x in df['headline']])\n",
    "# Convert headlines to sequences\n",
    "sequences = tokenizer.texts_to_sequences(df['headline'])\n",
    "print('sequneces',sequences[:5])           \n",
    "# Pad sequences to ensure uniform input size\n",
    "# \n",
    "# This is important for training neural network \n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_sent_len, padding='pre') \n",
    "print('padded_sequences', padded_sequences[:5]) \n",
    "# Convert labels to numpy array\n",
    "labels = df['is_sarcastic'].values \n",
    "print('labels', labels[:5])     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a953566a",
   "metadata": {},
   "source": [
    "### Code Explanation :\n",
    "\n",
    "-> import tensorflow\n",
    "   -> Imports the TensorFlow library.\n",
    "   -> Reason : TensorFlow is used to build and train deep learning models.\n",
    "   -> Purpose: Required for using Keras layers, preprocessing tools, and models.\n",
    "\n",
    "-> from tensorflow.keras.preprocessing.text import Tokenizer:Imports the Tokenizer class from Keras.\n",
    "   -> Reason : It tokenizes (converts) text into sequences of integers.\n",
    "   -> Purpose: To convert words to indices so that neural networks can process them.\n",
    "\n",
    "-> from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "   -> Purpose: This imports the pad_sequences function, which is used to ensure that all input sequences (lists \n",
    "               of token IDs) are the same length, which is required for training most deep learning models.\n",
    "\n",
    "-> tokenizer = Tokenizer()\n",
    "-> tokenizer.fit_on_texts(df['headline'])\n",
    "   -> Tokenizer(): Creates a tokenizer object. It’s used to convert text to a sequence of integers (each integer represents a word).\n",
    "   -> fit_on_texts(): Goes through all the headlines and builds a vocabulary (i.e., a mapping from each word to a unique integer).\n",
    "\n",
    "-> word_index = tokenizer.word_index\n",
    "-> vocab_size = len(word_index) + 1\n",
    "   -> word_index: A dictionary mapping words to their integer index.\n",
    "   -> vocab_size: Total number of unique words in your vocabulary.\n",
    "   -> +1 is added because Keras reserves index 0 (used for padding), so the actual vocab size needs to include that.\n",
    "\n",
    "-> max_sent_len = max([len(x) for x in df['headline']])\n",
    "   -> Purpose: Determines the maximum number of words in any single headline.\n",
    "   -> Reason : You need this to know how long to pad your sequences so that they’re all the same length.\n",
    "\n",
    "-> sequences = tokenizer.texts_to_sequences(df['headline'])\n",
    "-> print('sequences', sequences[:5])\n",
    "   -> texts_to_sequences(): Converts each headline (sentence) into a list of integers where each integer represents a word based on the tokenizer’s vocabulary.\n",
    "   -> Example: \"the dog barked\" → [1, 7, 56]\n",
    "\n",
    "-> padded_sequences = pad_sequences(sequences, maxlen=max_sent_len, padding='pre')\n",
    "-> print('padded_sequences', padded_sequences[:5])\n",
    "   -> pad_sequences(): Makes all sequences the same length by padding them.\n",
    "   -> maxlen=max_sent_len: Ensures every sequence is padded to the length of the longest sentence.\n",
    "   -> padding='pre': Adds zeros at the beginning of shorter sequences (e.g., [0, 0, 1, 7, 56]).\n",
    "   -> This is necessary because neural networks require inputs of uniform shape.\n",
    "\n",
    "-> labels = df['is_sarcastic'].values\n",
    "-> print('labels', labels[:5])\n",
    "   -> df['is_sarcastic']: This is the label column (0 = not sarcastic, 1 = sarcastic).\n",
    "   -> values: Converts the column to a NumPy array for training.\n",
    "   -> We need this format for model training in TensorFlow/Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7be93de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step-5 : Positional Encoding\n",
    "import numpy as np\n",
    "\n",
    "def get_positional_encoding(maxlen, d_model):\n",
    "    pos = np.arange(maxlen)[:, np.newaxis]\n",
    "    i = np.arange(d_model)[np.newaxis, :]\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    angle_rads = pos * angle_rates\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1f33c4",
   "metadata": {},
   "source": [
    "### Code Explanation :\n",
    "\n",
    "-> Purpose of Positional Encoding:\n",
    "   -> Transformers have no recurrence or convolution, so they need an explicit way to understand word order (position). Positional encoding injects position information into the input embeddings.\n",
    "\n",
    "-> def get_positional_encoding(maxlen, d_model):\n",
    "   -> maxlen: maximum number of tokens in a sequence (e.g., 30).\n",
    "   -> d_model: embedding dimension (e.g., 64 or 128).\n",
    "   -> The goal is to return a tensor of shape: (1, maxlen, d_model).\n",
    "\n",
    "-> pos = np.arange(maxlen)[:, np.newaxis]\n",
    "   -> np.arange(maxlen): creates a range like [0, 1, 2, ..., maxlen-1] → each row is a position index.\n",
    "   -> [:, np.newaxis]: reshapes to a column vector with shape (maxlen, 1).\n",
    "\n",
    "-> i = np.arange(d_model)[np.newaxis, :]\n",
    "-> Creates the list [0, 1, 2, ..., d_model-1] as a row.\n",
    "-> Shape becomes (1, d_model).\n",
    "-> Example (if d_model = 4):\n",
    "-> i = [[0, 1, 2, 3]]\n",
    "\n",
    "-> Compute angle rates (formula from the original Transformer paper)\n",
    "   -> angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "   -> This formula ensures that each dimension of the positional encoding follows a different frequency scale.\n",
    "   -> (i // 2): Even and odd indices share the same base frequency.\n",
    "   -> np.power(...): Applies the exponential denominator for smooth variation.\n",
    "\n",
    "-> Multiply positions with angle rates\n",
    "   -> angle_rads = pos * angle_rates\n",
    "   -> Element-wise multiplication.\n",
    "   -> Output shape: (maxlen, d_model)\n",
    "   -> This creates the raw angle values that will be passed through sin and cos.\n",
    "\n",
    "-> Apply sine to even indices\n",
    "   -> angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "   -> 0::2 selects all even columns (0, 2, 4, ...).\n",
    "   -> Applies sin() to these values.\n",
    "\n",
    "->  Apply cosine to odd indices\n",
    "    -> angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    -> 1::2 selects all odd columns (1, 3, 5, ...).\n",
    "    -> Applies cos() to these values.\n",
    "\n",
    "-> Add batch dimension and cast to Tensor\n",
    "   -> pos_encoding = angle_rads[np.newaxis, ...]\n",
    "   -> return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "   -> np.newaxis adds a batch dimension → shape becomes (1, maxlen, d_model)\n",
    "   -> tf.cast(...): Converts to a TensorFlow tensor of float32, ready to use in models.\n",
    "\n",
    "-> Final Output:\n",
    "   -> A Tensor of shape (1, maxlen, d_model) containing sinusoidal positional encodings. This is added to the word embeddings before feeding into a Transformer layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5cf8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Transformer Encoder Block\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(ff_dim, activation=\"relu\"),\n",
    "            layers.Dense(embed_dim),\n",
    "        ])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f8c3cd",
   "metadata": {},
   "source": [
    "### Code Explanation :\n",
    "\n",
    "-> Purpose of Transformer Encoder Block:\n",
    "   -> The Transformer encoder block captures contextual relationships between words using multi-head self-attention, followed by a feed-forward network (FFN). Each part includes normalization and residual connections to help with gradient flow and training.\n",
    "\n",
    "-> from tensorflow.keras import layers\n",
    "   -> Import Keras layers to build custom components like attention, normalization, etc.\n",
    "\n",
    "-> Class Definition and Initialization\n",
    "   -> class TransformerBlock(layers.Layer):\n",
    "   -> Creating a custom Keras layer that behaves like a Transformer encoder block.\n",
    "-> def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "   -> embed_dim: Dimensionality of the embeddings (e.g., 64 or 128).\n",
    "   -> num_heads: Number of attention heads.\n",
    "   -> ff_dim: Hidden layer size in the feed-forward network.\n",
    "   -> rate: Dropout rate to prevent overfitting.\n",
    "\n",
    "-> Multi-Head Attention Layer(att)\n",
    "   -> self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "   -> Creates a Multi-Head Self-Attention layer.\n",
    "   -> This lets the model look at the entire sequence at once and learn relationships between words, even far apart.\n",
    "   -> key_dim=embed_dim: dimension of each attention head's key vectors.\n",
    "\n",
    "-> Feed Forward Network (FFN)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(ff_dim, activation=\"relu\"),\n",
    "            layers.Dense(embed_dim),\n",
    "        ])\n",
    " -> A small 2-layer feed-forward network:\n",
    " -> First Dense layer expands dimension (ff_dim) and applies ReLU.\n",
    " -> Second Dense layer projects it back to embed_dim.\n",
    "\n",
    "-> Layer Normalization (helps training)\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "   -> Normalizes the input to each sub-layer (attention and FFN) to stabilize and speed up training.\n",
    "   -> epsilon=1e-6: a small number to avoid division by zero.\n",
    "\n",
    "-> Dropout (for regularization)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "   -> Dropout helps prevent overfitting by randomly turning off some neurons during training.\n",
    "   -> Applied after attention and FFN.\n",
    "\n",
    "-> Call Method (Forward Pass)\n",
    "    def call(self, inputs, training):\n",
    "   -> inputs: Tensor with shape (batch_size, seq_len, embed_dim)\n",
    "   -> training: Boolean flag for enabling/disabling dropout.\n",
    "\n",
    "-> Step 1: Self-Attention\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "   -> Applies multi-head attention to the input. Since it's self-attention, both query and value are the same (inputs).\n",
    "\n",
    "-> Step 2: Dropout after Attention\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "   -> Apply dropout only during training.\n",
    "\n",
    "-> Step 3: First Residual Connection + LayerNorm\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "   -> Residual connection: adds original inputs back to attention output.\n",
    "   -> Layer normalization helps with training stability.\n",
    "\n",
    "-> Step 4: Feed Forward Network\n",
    "    ffn_output = self.ffn(out1)\n",
    "   -> Passes the normalized output into a feed-forward network to further transform features.\n",
    "\n",
    "-> Step 5: Dropout after FFN\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "   -> Dropout after the FFN layer to prevent overfitting.\n",
    "\n",
    "->  Step 6: Second Residual Connection + LayerNorm\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "    -> Adds the FFN output back to out1 (which came from the attention block).\n",
    "    ->Final LayerNorm ensures smooth training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1727440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 911)]             0         \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 911, 64)           1658624   \n",
      "                                                                 \n",
      " tf.__operators__.add (TFOp  (None, 911, 64)           0         \n",
      " Lambda)                                                         \n",
      "                                                                 \n",
      " transformer_block (Transfo  (None, 911, 64)           41792     \n",
      " rmerBlock)                                                      \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 64)                0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 20)                1300      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 20)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1701737 (6.49 MB)\n",
      "Trainable params: 1701737 (6.49 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Build the full model\n",
    "embed_dim = 64\n",
    "num_heads = 2\n",
    "ff_dim = 64\n",
    "\n",
    "inputs = layers.Input(shape=(max_sent_len,))\n",
    "embedding_layer = layers.Embedding(vocab_size, embed_dim)(inputs)\n",
    "pos_encoding = get_positional_encoding(max_sent_len, embed_dim)\n",
    "x = embedding_layer + pos_encoding\n",
    "\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "x = transformer_block(x)\n",
    "\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(20, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9454a91",
   "metadata": {},
   "source": [
    "### Code Explanation : \n",
    "\n",
    "-> embed_dim: Dimension of the word embeddings and attention vectors.\n",
    "-> num_heads: Number of attention heads in the Transformer block.\n",
    "-> ff_dim: Dimension of the feed-forward network inside the Transformer.\n",
    "\n",
    "-> inputs = layers.Input(shape=(max_sent_len,))\n",
    "   -> Defines the input layer of the model.\n",
    "   -> Input shape is (max_sent_len,) — a sequence of integers (tokenized and padded text).\n",
    "\n",
    "-> embedding_layer = layers.Embedding(vocab_size, embed_dim)(inputs)\n",
    "   -> Creates an embedding layer that converts word indices into dense vectors of size embed_dim.\n",
    "   -> vocab_size: Total number of unique tokens.\n",
    "   -> embed_dim: Each word is mapped to a 64-dimensional vector.\n",
    "   ->  Output shape after this: (batch_size, max_sent_len, embed_dim)\n",
    "\n",
    "-> pos_encoding = get_positional_encoding(max_sent_len, embed_dim)\n",
    "   -> Calls your previously defined function to compute positional encodings.\n",
    "   -> Positional encoding adds information about word positions (since the Transformer doesn't have a built-in sense of order like RNNs or CNNs).\n",
    "\n",
    "-> x = embedding_layer + pos_encoding\n",
    "   -> Adds the positional encoding to the word embeddings.\n",
    "   -> Helps the model understand the order of words in the sequence.\n",
    "\n",
    "-> transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "   -> x = transformer_block(x)\n",
    "   -> Instantiates a custom Transformer encoder block and applies it to x.\n",
    "   -> This step uses multi-head attention to learn relationships between all tokens in the sequence.\n",
    "\n",
    "-> x = layers.GlobalAveragePooling1D()(x)\n",
    "   -> Averages the sequence of vectors (one vector per word) into a single vector (per example).\n",
    "   -> This reduces dimensionality and makes it easier to pass into dense layers.\n",
    "\n",
    "-> x = layers.Dropout(0.1)(x)\n",
    "   -> Applies dropout to prevent overfitting.\n",
    "\n",
    "-> x = layers.Dense(20, activation=\"relu\")(x)\n",
    "   -> A fully connected (dense) layer with 20 neurons.\n",
    "   -> Uses ReLU activation to introduce non-linearity.\n",
    "\n",
    "-> x = layers.Dropout(0.1)(x)\n",
    "   -> Another dropout layer for regularization.\n",
    "\n",
    "-> outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "   -> Final output layer with 1 neuron and a sigmoid activation.\n",
    "   -> Outputs a probability between 0 and 1 — perfect for binary classification (sarcastic vs. not sarcastic).\n",
    "\n",
    "-> model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "   -> Defines the full Keras Model with specified inputs and outputs.\n",
    "\n",
    "-> model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "   -> Compiles the model for training.\n",
    "\n",
    "-> optimizer=\"adam\": Adaptive optimizer — balances performance and speed.\n",
    "-> loss=\"binary_crossentropy\": Used for binary classification.\n",
    "-> metrics=[\"accuracy\"]: Track accuracy during training and validation.\n",
    "-> model.summary()\n",
    "   -> Prints a summary of the model architecture — including layer names, shapes, and parameter counts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95848e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "358/358 [==============================] - 731s 2s/step - loss: 0.6934 - accuracy: 0.5116 - val_loss: 0.6922 - val_accuracy: 0.5266\n",
      "Epoch 2/3\n",
      "358/358 [==============================] - 619s 2s/step - loss: 0.6922 - accuracy: 0.5229 - val_loss: 0.6919 - val_accuracy: 0.5266\n",
      "Epoch 3/3\n",
      "358/358 [==============================] - 618s 2s/step - loss: 0.6922 - accuracy: 0.5228 - val_loss: 0.6920 - val_accuracy: 0.5266\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Train the model (runs fast on CPU)\n",
    "history = model.fit(padded_sequences, labels, epochs=3, batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4df861",
   "metadata": {},
   "source": [
    "### Code Explanation : \n",
    "\n",
    "#### Train the Model : \n",
    "\n",
    "-> history = model.fit(padded_sequences, labels, epochs=3, batch_size=64, validation_split=0.2)\n",
    "   -> Purpose: Train the model using the input data and labels.\n",
    "\n",
    "-> model.fit(...): This function trains the model.\n",
    "-> padded_sequences: Input data (numerical representation of headlines, padded to same length).\n",
    "-> labels: Ground truth (0 = not sarcastic, 1 = sarcastic).\n",
    "-> epochs=3: Number of times the model will go through the entire dataset.\n",
    "-> batch_size=64: Number of samples the model will process before updating weights.\n",
    "-> validation_split=0.2: 20% of the data will be used for validation (to monitor performance on unseen data  during training).\n",
    "-> history: Stores training & validation accuracy/loss per epoch (used for plotting or analysis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b95bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "895/895 [==============================] - 273s 301ms/step - loss: 0.6922 - accuracy: 0.5236\n",
      "Accuracy: 0.52\n",
      "1/1 [==============================] - 155s 155s/step\n",
      "this is totally what I expected --> Sarcastic: 0 (Confidence: 0.48)\n",
      "the food was amazing --> Sarcastic: 0 (Confidence: 0.48)\n",
      "I just love being ignored --> Sarcastic: 0 (Confidence: 0.48)\n",
      "What a fantastic waste of time --> Sarcastic: 0 (Confidence: 0.48)\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Evaluate and predict\n",
    "loss, acc = model.evaluate(padded_sequences, labels)\n",
    "print(f\"Accuracy: {acc:.2f}\")\n",
    "\n",
    "sample_texts = [\n",
    "    \"this is totally what I expected\",\n",
    "    \"the food was amazing\",\n",
    "    \"I just love being ignored\",\n",
    "    \"What a fantastic waste of time\"\n",
    "]\n",
    "sample_seq = tokenizer.texts_to_sequences(sample_texts)\n",
    "sample_pad = pad_sequences(sample_seq, maxlen=max_sent_len, padding='post')\n",
    "predictions = model.predict(sample_pad)\n",
    "\n",
    "for text, pred in zip(sample_texts, predictions):\n",
    "    print(f\"{text} --> Sarcastic: {pred[0] > 0.5:.0f} (Confidence: {pred[0]:.2f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab4f63a",
   "metadata": {},
   "source": [
    "### Code Explanation : \n",
    "\n",
    "-> loss, acc = model.evaluate(padded_sequences, labels)\n",
    "   -> Purpose: Check how well the trained model performs on the full dataset.\n",
    "-> model.evaluate(...): Returns the loss and accuracy on the given data.\n",
    "-> padded_sequences: Input data used for evaluation.\n",
    "-> labels: Ground truth labels.\n",
    "-> loss: How bad the model's predictions are (lower is better).\n",
    "-> acc: Accuracy — percentage of correct predictions.\n",
    "\n",
    "-> print(f\"Accuracy: {acc:.2f}\")\n",
    "   -> Purpose: Print the model's overall accuracy in a readable format (e.g., Accuracy: 0.91 for 91%).\n",
    "\n",
    "->  Make Predictions on New Sentences\n",
    "sample_texts = [\n",
    "    \"this is totally what I expected\",\n",
    "    \"the food was amazing\",\n",
    "    \"I just love being ignored\",\n",
    "    \"What a fantastic waste of time\"\n",
    "]\n",
    "-> Purpose: Define new sample headlines to check if the model can classify them as sarcastic or not sarcastic.\n",
    "   -> These examples contain both genuine and sarcastic phrases.\n",
    "\n",
    "-> sample_seq = tokenizer.texts_to_sequences(sample_texts)\n",
    "   -> Purpose: Convert each text sample into a sequence of integers using the same tokenizer used for training.\n",
    "   -> This step maps each word to its corresponding index in the vocabulary.\n",
    "   -> Words not seen during training are replaced with an Out-Of-Vocabulary (OOV) token if configured.\n",
    "\n",
    "-> sample_pad = pad_sequences(sample_seq, maxlen=max_sent_len, padding='post')\n",
    "   -> Purpose: Pad sequences so all input samples have the same length (max_sent_len) as during training.\n",
    "   -> padding='post': Adds zeros after the sentence if it’s shorter than the max length.\n",
    "\n",
    "-> predictions = model.predict(sample_pad)\n",
    "   -> Purpose: Use the trained model to predict sarcasm probabilities for the sample inputs.\n",
    "   -> Returns an array of probabilities between 0 and 1.\n",
    "   -> Closer to 1 → likely sarcastic.\n",
    "   -> Closer to 0 → likely not sarcastic.\n",
    "\n",
    "->  Display Predictions Clearly\n",
    "for text, pred in zip(sample_texts, predictions):\n",
    "    print(f\"{text} --> Sarcastic: {pred[0] > 0.5:.0f} (Confidence: {pred[0]:.2f})\")\n",
    "-> Purpose: Loop through each sample and print whether the model classifies it as sarcastic or not.\n",
    "   -> pred[0] > 0.5: If prediction > 0.5, we consider it sarcastic (1), else not (0).\n",
    "   -> :.0f: Formats the sarcastic label as 0 or 1.\n",
    "   -> :.2f: Formats confidence score to 2 decimal places.\n",
    "-> Example output:\n",
    "this is totally what I expected --> Sarcastic: 1 (Confidence: 0.87)\n",
    "the food was amazing --> Sarcastic: 0 (Confidence: 0.10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c0a2e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\GEN_AI\\A_neural\\.venv\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\GEN_AI\\A_neural\\.venv\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:From d:\\GEN_AI\\A_neural\\.venv\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\GEN_AI\\A_neural\\.venv\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "716/716 [==============================] - 297s 401ms/step - loss: 0.4160 - accuracy: 0.8070 - val_loss: 0.3548 - val_accuracy: 0.8452\n",
      "Epoch 2/5\n",
      "716/716 [==============================] - 287s 400ms/step - loss: 0.1902 - accuracy: 0.9278 - val_loss: 0.3623 - val_accuracy: 0.8527\n",
      "Epoch 3/5\n",
      "716/716 [==============================] - 290s 405ms/step - loss: 0.0796 - accuracy: 0.9727 - val_loss: 0.4866 - val_accuracy: 0.8436\n",
      "Epoch 4/5\n",
      "716/716 [==============================] - 291s 406ms/step - loss: 0.0345 - accuracy: 0.9887 - val_loss: 0.6418 - val_accuracy: 0.8332\n",
      "Epoch 5/5\n",
      "716/716 [==============================] - 305s 427ms/step - loss: 0.0167 - accuracy: 0.9949 - val_loss: 0.8382 - val_accuracy: 0.8234\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1b3cc075330>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model = tf.keras.Sequential([\n",
    "    layers.Embedding(vocab_size, 64, input_length=max_sent_len),\n",
    "    layers.Bidirectional(layers.LSTM(64)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "lstm_model.fit(padded_sequences, labels, epochs=5, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b0af7b",
   "metadata": {},
   "source": [
    "### Code Explanation : \n",
    "\n",
    "-> Model Definition\n",
    "-> lstm_model = tf.keras.Sequential([\n",
    "   ->  Purpose: Start building a sequential model, where layers are stacked one after the other in a linear fashion.\n",
    "\n",
    "-> layers.Embedding(vocab_size, 64, input_length=max_sent_len),\n",
    "   -> Purpose: This layer converts word indices into dense vectors (embeddings).\n",
    "   -> vocab_size: Total number of unique words in the dataset (size of vocabulary).\n",
    "   -> 64: The dimension of the embedding for each word (each word becomes a 64-length vector).\n",
    "   -> input_length=max_sent_len: Specifies that each input sequence has max_sent_len tokens (padded if shorter).\n",
    "   -> Reason : Neural networks can't operate directly on word strings. This turns each word index into a vector that captures semantic meaning.\n",
    "\n",
    "-> layers.Bidirectional(layers.LSTM(64)),\n",
    "   -> Purpose: This adds a Bidirectional LSTM layer with 64 units.\n",
    "   -> LSTM(64): A standard Long Short-Term Memory (LSTM) unit with 64 memory cells.\n",
    "   -> Bidirectional(...): Runs the LSTM both forward and backward, capturing information from past and future context.\n",
    "   -> Reason : Bidirectional LSTMs are powerful for understanding text, especially when word order and context matter.\n",
    "-> layers.Dense(64, activation='relu'),\n",
    "   ->  Purpose: A fully connected (Dense) layer with 64 neurons and ReLU activation.\n",
    "   -> activation='relu': Introduces non-linearity, helping the model learn complex relationships.\n",
    "   -> Reason : After LSTM extracts temporal features, this layer helps in further learning patterns before the final prediction.\n",
    "\n",
    "-> layers.Dense(1, activation='sigmoid')])\n",
    "   -> Purpose: The final output layer.\n",
    "   -> 1: Outputting a single number between 0 and 1.\n",
    "   -> activation='sigmoid': Used for binary classification (sarcastic or not).\n",
    "   -> Reason : The sigmoid output can be interpreted as a probability of the input being sarcastic.\n",
    "\n",
    "->  Model Compilation\n",
    "    -> lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    -> Purpose: Configure the model for training.\n",
    "    -> optimizer='adam': Efficient optimization algorithm for faster convergence.\n",
    "    -> loss='binary_crossentropy': Appropriate loss function for binary classification tasks.\n",
    "    -> metrics=['accuracy']: Track the model's accuracy during training and evaluation.\n",
    "\n",
    "-> Model Training\n",
    "   -> lstm_model.fit(padded_sequences, labels, epochs=5, validation_split=0.2)\n",
    "   -> Purpose: Train the model using your data.\n",
    "   -> padded_sequences: Input data (tokenized and padded headlines).\n",
    "   -> labels: Corresponding sarcasm labels (0 or 1).\n",
    "   -> epochs=5: Train for 5 full passes through the data.\n",
    "   -> validation_split=0.2: Use 20% of the data to validate model performance after each epoch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed858301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "895/895 [==============================] - 87s 97ms/step - loss: 0.1747 - accuracy: 0.9628\n",
      "Accuracy: 0.96\n",
      "1/1 [==============================] - 1s 795ms/step\n",
      "this is totally what I expected --> Sarcastic: 0 (Confidence: 0.06)\n",
      "the food was amazing --> Sarcastic: 0 (Confidence: 0.06)\n",
      "I just love being ignored --> Sarcastic: 0 (Confidence: 0.06)\n",
      "What a fantastic waste of time --> Sarcastic: 0 (Confidence: 0.06)\n"
     ]
    }
   ],
   "source": [
    "loss, acc = lstm_model.evaluate(padded_sequences, labels)\n",
    "print(f\"Accuracy: {acc:.2f}\")\n",
    "\n",
    "sample_texts = [\n",
    "    \"this is totally what I expected\",\n",
    "    \"the food was amazing\",\n",
    "    \"I just love being ignored\",\n",
    "    \"What a fantastic waste of time\"\n",
    "]\n",
    "sample_seq1 = tokenizer.texts_to_sequences(sample_texts)\n",
    "sample_pad1 = pad_sequences(sample_seq1, maxlen=max_sent_len, padding='post')\n",
    "predictions = lstm_model.predict(sample_pad1)\n",
    "\n",
    "for text, pred in zip(sample_texts, predictions):\n",
    "    print(f\"{text} --> Sarcastic: {pred[0] > 0.5:.0f} (Confidence: {pred[0]:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9077f04d",
   "metadata": {},
   "source": [
    "### Code Explanation :\n",
    "\n",
    "-> Model Evaluation\n",
    "   -> loss, acc = lstm_model.evaluate(padded_sequences, labels)\n",
    "   -> Purpose: Evaluate the model’s performance on the entire dataset.\n",
    "   -> padded_sequences: The input headlines (already tokenized and padded).\n",
    "   -> labels: The corresponding binary labels (0 = not sarcastic, 1 = sarcastic).\n",
    "   -> evaluate(...) returns:\n",
    "   -> loss: How far the predictions are from the actual labels.\n",
    "   -> acc: Accuracy of the model.\n",
    "\n",
    "-> print(f\"Accuracy: {acc:.2f}\")\n",
    "   ->  Purpose: Prints the accuracy as a percentage with 2 decimal places.\n",
    "   ->  Example: Accuracy: 0.91 means 91% correct predictions.\n",
    "\n",
    "-> Prediction on Sample Inputs\n",
    "sample_texts = [\n",
    "    \"this is totally what I expected\",\n",
    "    \"the food was amazing\",\n",
    "    \"I just love being ignored\",\n",
    "    \"What a fantastic waste of time\"\n",
    "]\n",
    "\n",
    " -> Purpose: These are custom sentences you want to test for sarcasm.\n",
    " -> A mix of clearly sarcastic and sincere phrases.\n",
    "\n",
    "-> sample_seq1 = tokenizer.texts_to_sequences(sample_texts)\n",
    "  -> Purpose: Converts the raw text into sequences of word indices using the same tokenizer you trained on.\n",
    "  -> \"this is totally what I expected\" → [42, 7, 58, 13, 31, 267] (for example)\n",
    "  -> Reason : Neural networks don’t work directly with text; they need numerical input.\n",
    "\n",
    "-> sample_pad1 = pad_sequences(sample_seq1, maxlen=max_sent_len, padding='post')\n",
    "   -> Purpose: Pads/truncates each sequence so they all have the same length as your training input (max_sent_len).\n",
    "   -> padding='post': Adds zeros after the actual word indices if the sentence is too short.\n",
    "   -> Reason : Models expect a fixed-size input for each example.\n",
    "\n",
    "-> predictions = lstm_model.predict(sample_pad1)\n",
    "   -> Purpose: Uses the trained model to predict sarcasm for each padded input.\n",
    "   -> Returns a list of values between 0 and 1 for each input sentence.\n",
    "   -> Example: [0.91], [0.02], [0.88], [0.95]\n",
    "   -> The closer to 1 → more sarcastic. Closer to 0 → more sincere.\n",
    "\n",
    "-> Print Predictions\n",
    "for text, pred in zip(sample_texts, predictions):\n",
    "    print(f\"{text} --> Sarcastic: {pred[0] > 0.5:.0f} (Confidence: {pred[0]:.2f})\")\n",
    "  -> Purpose: Nicely formats and displays the prediction for each input sentence.\n",
    "  -> pred[0] > 0.5: Checks if the prediction is above 0.5 (sarcastic if True).\n",
    "  ->:.0f: Rounds the boolean to 0 (no) or 1 (yes).\n",
    "  -> :.2f: Shows the prediction confidence up to two decimals.\n",
    "-> Example Output:\n",
    "this is totally what I expected --> Sarcastic: 1 (Confidence: 0.91)\n",
    "the food was amazing --> Sarcastic: 0 (Confidence: 0.08)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
